# # AIOps Agent - è‡ªåŠ¨åŒ–è¿ç»´æ•…éšœæ’æŸ¥
# # ä½¿ç”¨ Planner-Operation-Reflection æ¨¡å¼
#
# from langchain_community.chat_models import ChatTongyi
# from typing import List, TypedDict, Annotated, Literal
# from loguru import logger
# from langgraph.graph import StateGraph, END
# from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
# import operator
# import json
#
#
# # ==================== State å®šä¹‰ ====================
#
# class AIOpsState(TypedDict):
#     """AIOps Agent çŠ¶æ€å®šä¹‰
#
#     State åœ¨ Planner-Operation-Reflection å¾ªç¯ä¸­ä¼ é€’ï¼Œä¿å­˜æ‰€æœ‰å¿…è¦ä¿¡æ¯
#     """
#     # ç”¨æˆ·è¾“å…¥
#     input: str  # ç”¨æˆ·çš„é—®é¢˜æˆ–å‘Šè­¦æè¿°
#
#     # æ’æŸ¥è®¡åˆ’
#     plan: str  # Planner ç”Ÿæˆçš„æ’æŸ¥è®¡åˆ’ï¼ˆæ­¥éª¤åˆ—è¡¨ï¼‰
#
#     # æ‰§è¡Œå†å²ï¼ˆä½¿ç”¨ operator.add è‡ªåŠ¨åˆå¹¶ï¼‰
#     past_steps: Annotated[List[str], operator.add]  # å·²æ‰§è¡Œçš„æ­¥éª¤å’Œç»“æœ
#
#     # å¾ªç¯æ§åˆ¶
#     iteration: int  # å½“å‰å¾ªç¯æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼Œæœ€å¤§3æ¬¡ï¼‰
#
#     # æœ€ç»ˆè¾“å‡º
#     response: str  # æœ€ç»ˆçš„åˆ†ææŠ¥å‘Š
#
#
# # ==================== AIOps Agent ç±» ====================
#
# class AIOpsAgent:
#     """AIOps Agent - è‡ªåŠ¨åŒ–æ•…éšœæ’æŸ¥å’Œæ ¹å› åˆ†æ
#
#     ä½¿ç”¨ Planner-Operation-Reflection æ¨¡å¼:
#     - Planner: åˆ¶å®šæ’æŸ¥è®¡åˆ’
#     - Operation: æ‰§è¡Œæ“ä½œï¼ˆè°ƒç”¨å·¥å…·ï¼‰
#     - Reflection: åæ€è¯„ä¼°ï¼Œå†³å®šæ˜¯å¦ç»§ç»­
#     """
#
#     def __init__(self, api_key: str, model: str, tools: List):
#         """
#         åˆå§‹åŒ– AIOps Agent
#
#         å‚æ•°:
#             api_key: DashScope API Key
#             model: æ¨¡å‹åç§°ï¼Œå¦‚ "qwen-max"
#             tools: å·¥å…·åˆ—è¡¨ [query_prometheus_alerts, query_log, query_internal_docs, get_current_datetime]
#         """
#         # åˆå§‹åŒ– LLM å¹¶ç»‘å®šå·¥å…·
#         self.llm = ChatTongyi(
#             dashscope_api_key=api_key,
#             model_name=model,
#             streaming=False
#         ).bind_tools(tools)
#
#         # ä¿å­˜å·¥å…·å­—å…¸ï¼ˆæ–¹ä¾¿åç»­è°ƒç”¨ï¼‰
#         self.tools = {tool.name: tool for tool in tools}
#
#         # åˆ›å»º Graph
#         self.graph = self._create_graph()
#
#         logger.info("AIOpsAgent åˆå§‹åŒ–æˆåŠŸ")
#
#     def _create_graph(self):
#         """åˆ›å»º LangGraph å·¥ä½œæµ
#
#         å·¥ä½œæµ: Planner â†’ Operation â†’ Reflection â†’ (ç»§ç»­ or ç»“æŸ)
#         """
#         # TODO: å®šä¹‰èŠ‚ç‚¹å‡½æ•°
#         # TODO: æ„å»ºå›¾
#         workflow = StateGraph(AIOpsState)
#         workflow.add_node("planner", self.planner_node)
#         workflow.add_node("operation", self.operation_node)
#         workflow.add_node("reflection", self.reflection_node)
#         workflow.set_entry_point("planner")
#         workflow.add_edge("planner", "operation")
#         workflow.add_edge("operation", "reflection")
#         workflow.add_conditional_edges(
#             "reflection",
#             self.should_continue,
#             {
#                 "continue": "planner",
#                 "end": END
#             }
#         )
#         return workflow.compile()
#
#     # ==================== èŠ‚ç‚¹å‡½æ•° ====================
#
#     async def planner_node(self, state: AIOpsState):
#         """Planner - åˆ¶å®šæ’æŸ¥è®¡åˆ’
#
#         åŠŸèƒ½:
#         - åˆ†æé—®é¢˜
#         - åˆ¶å®šæ’æŸ¥æ­¥éª¤
#         - å†³å®šè°ƒç”¨å“ªäº›å·¥å…·
#         """
#         logger.info("ğŸ§  Planner: å¼€å§‹åˆ¶å®šæ’æŸ¥è®¡åˆ’")
#
#         # 1. è·å–å½“å‰çŠ¶æ€
#         user_input = state["input"]
#         past_steps = state.get("past_steps", [])
#         iteration = state.get("iteration", 0)
#
#         # 2. æ„å»º Prompt
#         if iteration == 0:
#             # ç¬¬ä¸€æ¬¡è§„åˆ’
#             prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£åˆ¶å®šæ•…éšœæ’æŸ¥è®¡åˆ’ã€‚
#
# ç”¨æˆ·é—®é¢˜ï¼š{user_input}
#
# è¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œ**ä»…åˆ¶å®šä¸‹ä¸€æ­¥**æœ€å…³é”®çš„ 1-2 ä¸ªæ’æŸ¥åŠ¨ä½œã€‚
# - å¦‚æœå·²ç»æŸ¥è¯¢è¿‡å‘Šè­¦ä¸”æ²¡æœ‰æ˜ç¡®çº¿ç´¢ï¼Œ**å¿…é¡»**è½¬å‘æŸ¥è¯¢æ—¥å¿—(query_log)æˆ–çŸ¥è¯†åº“(query_internal_docs)ã€‚
# **å†³ç­–é€»è¾‘**ï¼š
# 1. ã€å…³é”®ã€‘å¦‚æœä¹‹å‰çš„æ—¥å¿—æŸ¥è¯¢ç»“æœä¸­å·²ç»åŒ…å«äº† **Error, Exception, Timeout** ç­‰æ˜ç¡®æŠ¥é”™ï¼š
#    - ä¸‹ä¸€æ­¥å¿…é¡»æ˜¯ï¼šè°ƒç”¨ `query_internal_docs` æŸ¥è¯¢è¯¥æŠ¥é”™çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ–è€…ç›´æ¥è¯´æ˜â€œä¿¡æ¯å……è¶³â€ã€‚
#    - **ä¸¥ç¦**å†å›å¤´å»æŸ¥ç›‘æ§/å‘Šè­¦ã€‚
#
# 2. å¦‚æœæ—¥å¿—æ²¡é—®é¢˜ï¼Œä½†å‘Šè­¦è¿˜åœ¨ï¼š
#    - å°è¯•æ‰©å¤§æ—¥å¿—æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´ï¼Œæˆ–æŸ¥è¯¢å…³è”æœåŠ¡çš„æ—¥å¿—ã€‚
#
# 3. **ç»å¯¹ç¦æ­¢**ï¼š
#    - ä¸¥ç¦é‡å¤è°ƒç”¨ `query_prometheus_alerts`ï¼ˆé™¤éä½ è®¤ä¸ºå‘Šè­¦çŠ¶æ€å‘ç”Ÿäº†å‰§çƒˆå˜åŒ–ï¼Œä½†è¿™å¾ˆå°‘è§ï¼‰ã€‚
#    - ä¸¥ç¦é‡å¤æ‰§è¡Œå®Œå…¨ç›¸åŒçš„ query_log å‚æ•°ã€‚
#
# å¯ç”¨å·¥å…·ï¼š
# - query_prometheus_alerts: æŸ¥è¯¢ Prometheus å‘Šè­¦
# - query_log: æŸ¥è¯¢ç³»ç»Ÿæ—¥å¿—ï¼ˆå‚æ•°ï¼šquery, time_rangeï¼‰
# - query_internal_docs: æŸ¥è¯¢è¿ç»´çŸ¥è¯†åº“ï¼ˆå‚æ•°ï¼šqueryï¼‰
# - get_current_datetime: è·å–å½“å‰æ—¶é—´
#
# è¯·ä»¥æ¸…æ™°çš„æ­¥éª¤åˆ—è¡¨å½¢å¼è¾“å‡ºè®¡åˆ’ã€‚"""
#         else:
#             # é‡æ–°è§„åˆ’ï¼ˆReplanï¼‰
#             past_steps_str = "\n".join(past_steps)
#             prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£åˆ¶å®šæ•…éšœæ’æŸ¥è®¡åˆ’ã€‚
#
# ç”¨æˆ·é—®é¢˜ï¼š{user_input}
#
# å·²æ‰§è¡Œçš„æ­¥éª¤ï¼š
# {past_steps_str}
#
# è¯·æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œ**ä»…åˆ¶å®šä¸‹ä¸€æ­¥**æœ€å…³é”®çš„ 1-2 ä¸ªæ’æŸ¥åŠ¨ä½œã€‚
# - å¦‚æœå·²ç»æŸ¥è¯¢è¿‡å‘Šè­¦ä¸”æ²¡æœ‰æ˜ç¡®çº¿ç´¢ï¼Œ**å¿…é¡»**è½¬å‘æŸ¥è¯¢æ—¥å¿—(query_log)æˆ–çŸ¥è¯†åº“(query_internal_docs)ã€‚
# **å†³ç­–é€»è¾‘**ï¼š
# 1. ã€å…³é”®ã€‘å¦‚æœä¹‹å‰çš„æ—¥å¿—æŸ¥è¯¢ç»“æœä¸­å·²ç»åŒ…å«äº† **Error, Exception, Timeout** ç­‰æ˜ç¡®æŠ¥é”™ï¼š
#    - ä¸‹ä¸€æ­¥å¿…é¡»æ˜¯ï¼šè°ƒç”¨ `query_internal_docs` æŸ¥è¯¢è¯¥æŠ¥é”™çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ–è€…ç›´æ¥è¯´æ˜â€œä¿¡æ¯å……è¶³â€ã€‚
#    - **ä¸¥ç¦**å†å›å¤´å»æŸ¥ç›‘æ§/å‘Šè­¦ã€‚
#
# 2. å¦‚æœæ—¥å¿—æ²¡é—®é¢˜ï¼Œä½†å‘Šè­¦è¿˜åœ¨ï¼š
#    - å°è¯•æ‰©å¤§æ—¥å¿—æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´ï¼Œæˆ–æŸ¥è¯¢å…³è”æœåŠ¡çš„æ—¥å¿—ã€‚
#
# 3. **ç»å¯¹ç¦æ­¢**ï¼š
#    - ä¸¥ç¦é‡å¤è°ƒç”¨ `query_prometheus_alerts`ï¼ˆé™¤éä½ è®¤ä¸ºå‘Šè­¦çŠ¶æ€å‘ç”Ÿäº†å‰§çƒˆå˜åŒ–ï¼Œä½†è¿™å¾ˆå°‘è§ï¼‰ã€‚
#    - ä¸¥ç¦é‡å¤æ‰§è¡Œå®Œå…¨ç›¸åŒçš„ query_log å‚æ•°ã€‚
#
# å¯ç”¨å·¥å…·ï¼š
# - query_prometheus_alerts: æŸ¥è¯¢ Prometheus å‘Šè­¦
# - query_log: æŸ¥è¯¢ç³»ç»Ÿæ—¥å¿—ï¼ˆå‚æ•°ï¼šquery, time_rangeï¼‰
# - query_internal_docs: æŸ¥è¯¢è¿ç»´çŸ¥è¯†åº“ï¼ˆå‚æ•°ï¼šqueryï¼‰
# - get_current_datetime: è·å–å½“å‰æ—¶é—´
#
# è¯·ä»¥æ¸…æ™°çš„æ­¥éª¤åˆ—è¡¨å½¢å¼è¾“å‡ºè®¡åˆ’ã€‚"""
#
#         # 3. è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’
#         messages = [
#             SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AIOps æ•…éšœæ’æŸ¥ä¸“å®¶ã€‚"),
#             HumanMessage(content=prompt)
#         ]
#
#         response = await self.llm.ainvoke(messages)
#         plan = response.content
#
#         logger.info(f"ğŸ“‹ Planner ç”Ÿæˆè®¡åˆ’:\n{plan}")
#
#         # 4. æ›´æ–° State
#         return {
#             "plan": plan,
#             "iteration": iteration + 1
#         }
#
#     async def operation_node(self, state: AIOpsState):
#         """Operation - æ‰§è¡Œæ“ä½œ
#
#         åŠŸèƒ½:
#         - æ‰§è¡Œè®¡åˆ’ä¸­çš„æ­¥éª¤
#         - è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯
#         - è®°å½•æ‰§è¡Œç»“æœ
#         """
#         logger.info("âš™ï¸ Operation: å¼€å§‹æ‰§è¡Œæ“ä½œ")
#
#         # 1. è·å–è®¡åˆ’
#         plan = state["plan"]
#         user_input = state["input"]
#         past_steps = state.get("past_steps", [])
#         context_str = "\n".join(past_steps[-3:]) if past_steps else "æ— ï¼ˆé¦–æ¬¡æ‰§è¡Œï¼‰"
#         # 2. æ„å»º Promptï¼ˆè®© LLM æ ¹æ®è®¡åˆ’è°ƒç”¨å·¥å…·ï¼‰
#         prompt = f"""ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ‰§è¡ŒåŠ©æ‰‹ã€‚
#
# ç”¨æˆ·é—®é¢˜ï¼š{user_input}
#
# ã€é‡è¦å‚è€ƒä¸Šä¸‹æ–‡ã€‘
# (è¿™æ˜¯ä¹‹å‰å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œè¯·ä»ä¸­æå– Podåç§°ã€æ—¶é—´ã€é”™è¯¯ä¿¡æ¯ç­‰ç”¨äºåç»­å·¥å…·çš„å‚æ•°)ï¼Œå°½é‡ä¸é‡å¤æ‰§è¡Œä¹‹å‰çš„å·¥å…·ï¼Œé™¤éå‚æ•°ä¸åŒ:
# {context_str}
#
# æ’æŸ¥è®¡åˆ’ï¼š
# {plan}
#
# è¯·æ ¹æ®è®¡åˆ’è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚ä½ å¿…é¡»è°ƒç”¨ä»¥ä¸‹å·¥å…·ä¹‹ä¸€ï¼š
# - query_prometheus_alerts (): æŸ¥è¯¢å½“å‰å‘Šè­¦ï¼ˆæ— éœ€å‚æ•°ï¼‰
# - get_current_datetime (): è·å–å½“å‰æ—¶é—´ï¼ˆæ— éœ€å‚æ•°ï¼‰
# - query_prometheus_alerts (æ— å‚æ•°)
# - query_log (query: str, time_range: str):æŸ¥è¯¢æ—¥å¿—
# - query_internal_docs (query: str)ï¼šæŸ¥è¯¢çŸ¥è¯†åº“
# **é‡è¦**ï¼šè¯·**ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è®¡åˆ’**è°ƒç”¨å¯¹åº”çš„å·¥å…·ã€‚
# - å¦‚æœè®¡åˆ’ä¸­æåˆ°æŸ¥è¯¢æ—¥å¿—ï¼Œè¯·åŠ¡å¿…è°ƒç”¨ `query_log` å¹¶å¡«å…¥åˆç†çš„ query å‚æ•°å’Œtime_rangeå‚æ•°ã€‚
# - å¦‚æœè®¡åˆ’ä¸­æåˆ°æŸ¥çŸ¥è¯†åº“ï¼Œè¯·åŠ¡å¿…è°ƒç”¨ `query_internal_docs`å¹¶å¡«å…¥åˆç†çš„queryå‚æ•°ã€‚
# - ä¸è¦è‡ªè¡Œå†³å®šè·³è¿‡ä»»ä½•æ­¥éª¤ã€‚"""
#
#         messages = [
#             SystemMessage(content="ä½ æ˜¯ä¸€ä¸ª AIOps æ‰§è¡ŒåŠ©æ‰‹ï¼Œè´Ÿè´£è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯ã€‚"),
#             HumanMessage(content=prompt)
#         ]
#
#         # 3. è°ƒç”¨ LLM
#         response = await self.llm.ainvoke(messages)
#
#         # 4. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
#         if not hasattr(response, 'tool_calls') or not response.tool_calls:
#             # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å› LLM çš„å›ç­”
#             logger.warning("âš ï¸ Operation: LLM æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·")
#             return {
#                 "past_steps": [f"æ‰§è¡Œç»“æœ: {response.content}"]
#             }
#
#         # 5. æ‰§è¡Œæ‰€æœ‰å·¥å…·
#         logger.info(f"ğŸ”§ Operation: æ‰§è¡Œ {len(response.tool_calls)} ä¸ªå·¥å…·")
#
#         results = []
#         for tool_call in response.tool_calls:
#             tool_name = tool_call["name"]
#             tool_args = tool_call["args"]
#
#             logger.info(f"  - è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
#
#             try:
#                 # è·å–å·¥å…·å¹¶æ‰§è¡Œ
#                 tool = self.tools[tool_name]
#                 result = await tool.ainvoke(tool_args)  # â† å…³é”®ï¼šéœ€è¦ await
#
#                 # è®°å½•ç»“æœï¼ˆæˆªæ–­è¿‡é•¿ç»“æœï¼‰
#                 step_result = f"å·¥å…·: {tool_name}\nå‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}\nç»“æœ: {result[:500]}..."
#                 results.append(step_result)
#
#                 logger.info(f"  âœ… {tool_name} æ‰§è¡ŒæˆåŠŸ")
#             except Exception as e:
#                 error_msg = f"å·¥å…·: {tool_name}\né”™è¯¯: {str(e)}"
#                 results.append(error_msg)
#                 logger.error(f"  âŒ {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
#
#         # 6. è¿”å›ç»“æœï¼ˆä¼šè‡ªåŠ¨æ·»åŠ åˆ° past_stepsï¼‰
#         return {
#             "past_steps": results
#         }
#
#     async def reflection_node(self, state: AIOpsState):
#         """Reflection - åæ€è¯„ä¼°
#
#         åŠŸèƒ½:
#         - è¯„ä¼°æ‰§è¡Œç»“æœ
#         - åˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……è¶³
#         - å¦‚æœå……è¶³ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
#         - å¦‚æœä¸è¶³ï¼Œä¸åšä»»ä½•æ“ä½œï¼ˆè®© should_continue å†³å®šï¼‰
#         """
#         logger.info("ğŸ¤” Reflection: å¼€å§‹è¯„ä¼°ç»“æœ")
#
#         # 1. è·å–çŠ¶æ€
#         user_input = state["input"]
#         past_steps = state.get("past_steps", [])
#         iteration = state.get("iteration", 0)
#
#         # 2. æ„å»º Prompt
#         past_steps_str = "\n\n".join(past_steps)
#         prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°æ•…éšœæ’æŸ¥ç»“æœã€‚
#
# ç”¨æˆ·é—®é¢˜ï¼š{user_input}
#
# å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
# {past_steps_str}
#
# **åˆ¤å®šæ ‡å‡†ï¼ˆé‡è¦ï¼‰**ï¼š
# 1. **åªè¦**åœ¨æ—¥å¿—ä¸­å‘ç°äº†æ˜ç¡®çš„**é”™è¯¯å †æ ˆã€å¼‚å¸¸ç±»åï¼ˆExceptionï¼‰æˆ–è¶…æ—¶ï¼ˆTimeoutï¼‰ä¿¡æ¯**ï¼Œå°±è§†ä¸º**æ ¹å› å·²æ‰¾åˆ°**ã€‚æ­¤æ—¶**å¿…é¡»**ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚
# 2. ä¸éœ€è¦ç­‰å¾…æ‰€æœ‰ç»†èŠ‚éƒ½å®Œç¾ï¼Œåªè¦èƒ½è§£é‡Šå‘Šè­¦åŸå› ï¼ˆä¾‹å¦‚ï¼šCPUé«˜æ˜¯å› ä¸ºæ•°æ®åº“è¿æ¥æ± å¡æ­»ï¼‰å³å¯ã€‚
#
# è¯·è¯„ä¼°å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿè¿›è¡Œæ ¹å› åˆ†æï¼š
#
# 1. å¦‚æœä¿¡æ¯å……è¶³(ç¬¦åˆä¸Šè¿°â€œæ ¹å› å·²æ‰¾åˆ°â€çš„æ ‡å‡†)ï¼š
#    - è¯·ç”Ÿæˆè¯¦ç»†çš„æ•…éšœåˆ†ææŠ¥å‘Š
#    - æŠ¥å‘Šæ ¼å¼ï¼šé—®é¢˜æè¿°ã€æ ¹å› åˆ†æã€è§£å†³å»ºè®®
#    - ä»¥"ã€æœ€ç»ˆæŠ¥å‘Šã€‘"å¼€å¤´
#
# 2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼š
#    - ç®€å•è¯´æ˜"ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç»§ç»­æ’æŸ¥"
#    - ä¸è¦ç»™å‡ºå…·ä½“å»ºè®®ï¼ˆPlanner ä¼šé‡æ–°è§„åˆ’ï¼‰
#    **é‡è¦**ï¼šå¦‚æœå‘ç°æœ€è¿‘çš„æ­¥éª¤ä¸€ç›´åœ¨é‡å¤æŸ¥è¯¢ç›¸åŒå†…å®¹ï¼ˆä¾‹å¦‚é‡å¤æŸ¥å‘Šè­¦ï¼‰ï¼Œè¯·åœ¨è¯„ä¼°ä¸­æ˜ç¡®æŒ‡å‡ºâ€˜éœ€è¦å°è¯•æ–°çš„æ’æŸ¥æ–¹å‘ï¼Œå¦‚æŸ¥è¯¢æ—¥å¿—/çŸ¥è¯†åº“/æ—¶é—´â€™ï¼Œä»¥ä¾¿å¼•å¯¼ Plannerã€‚
#    """
#
#         # 3. è°ƒç”¨ LLM
#         messages = [
#             SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AIOps æ•…éšœåˆ†æä¸“å®¶ã€‚"),
#             HumanMessage(content=prompt)
#         ]
#
#         response = await self.llm.ainvoke(messages)
#         evaluation = response.content
#
#         logger.info(f"ğŸ“Š Reflection è¯„ä¼°:\n{evaluation[:200]}...")
#
#         # 4. å¦‚æœç”Ÿæˆäº†æœ€ç»ˆæŠ¥å‘Šï¼Œä¿å­˜åˆ° response
#         if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in evaluation:
#             logger.info("âœ… Reflection: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
#             return {
#                 "response": evaluation
#             }
#         else:
#             # ä¿¡æ¯ä¸è¶³ï¼Œä¸æ›´æ–° responseï¼ˆä¿æŒä¸ºç©ºï¼‰
#             logger.info("ğŸ”„ Reflection: ä¿¡æ¯ä¸è¶³")
#             return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä¸æ›´æ–°ä»»ä½•å­—æ®µ
#
#     # ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================
#
#     def should_continue(self, state: AIOpsState) -> Literal["continue", "end"]:
#         """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯
#
#         å†³ç­–é€»è¾‘ï¼š
#         1. å¦‚æœ response ä¸ä¸ºç©º â†’ å·²ç”ŸæˆæŠ¥å‘Š â†’ end
#         2. å¦‚æœ iteration >= 6 â†’ è¾¾åˆ°ä¸Šé™ â†’ endï¼ˆå¼ºåˆ¶ï¼‰
#         3. å¦åˆ™ â†’ continue
#
#         è¿”å›:
#             "continue": å›åˆ° Planner ç»§ç»­æ’æŸ¥
#             "end": ç»“æŸæµç¨‹
#         """
#         response = state.get("response", "")
#         iteration = state.get("iteration", 0)
#
#         # 1. å¦‚æœå·²ç»ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œç»“æŸ
#         if response:
#             logger.info("ğŸ¯ å†³ç­–: å·²ç”ŸæˆæŠ¥å‘Šï¼Œç»“æŸæµç¨‹")
#             return "end"
#
#         # 2. å¦‚æœè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
#         if iteration >= 6:
#             logger.warning("âš ï¸ å†³ç­–: è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°(6æ¬¡)ï¼Œå¼ºåˆ¶ç»“æŸ")
#             return "end"
#
#         # 3. å¦åˆ™ç»§ç»­
#         logger.info(f"ğŸ”„ å†³ç­–: ç»§ç»­æ’æŸ¥ï¼ˆå½“å‰ç¬¬ {iteration} è½®ï¼‰")
#         return "continue"
#
#     # ==================== å¯¹å¤–æ¥å£ ====================
#
#     async def analyze(self, problem: str) -> str:
#         """
#         åˆ†ææ•…éšœå¹¶ç”ŸæˆæŠ¥å‘Š
#
#         å‚æ•°:
#             problem: æ•…éšœæè¿°æˆ–å‘Šè­¦ä¿¡æ¯
#
#         è¿”å›:
#             åˆ†ææŠ¥å‘Š
#         """
#         try:
#             logger.info(f"ğŸš€ AIOps Agent å¼€å§‹åˆ†æé—®é¢˜: {problem[:100]}...")
#             initial_state = {
#                 "input": problem,
#                 "plan": "",
#                 "past_steps": [],
#                 "iteration": 0,
#                 "response": ""
#             }
#             result = await self.graph.ainvoke(initial_state)
#
#             final_report = result.get("response", "")
#
#             if not final_report:
#                 # å¦‚æœæ²¡æœ‰ç”ŸæˆæŠ¥å‘Šï¼ˆè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•æŠ¥å‘Š
#                 logger.warning("âš ï¸ æœªç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå¯èƒ½è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°")
#                 past_steps_str = "\n\n".join(result.get("past_steps", []))
#                 final_report = f"""ã€åˆ†ææŠ¥å‘Šã€‘
#                 é—®é¢˜æè¿°ï¼š{problem}
#
# å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
# {past_steps_str}
#
# æ³¨æ„ï¼šç”±äºè¾¾åˆ°æœ€å¤§æ’æŸ¥æ¬¡æ•°é™åˆ¶ï¼Œåˆ†æå¯èƒ½ä¸å®Œæ•´ã€‚å»ºè®®äººå·¥ä»‹å…¥è¿›ä¸€æ­¥æ’æŸ¥ã€‚"""
#             logger.info("âœ… AIOps Agent åˆ†æå®Œæˆ")
#             return final_report
#         except Exception as e:
#             logger.error(f"âŒ AIOps Agent åˆ†æå¤±è´¥: {str(e)}")
#             import traceback
#             logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
#             raise
# AIOps Agent - è‡ªåŠ¨åŒ–è¿ç»´æ•…éšœæ’æŸ¥
# æ¶æ„æ¨¡å¼: Planner (è®¡æ•°å™¨çŠ¶æ€æœº) -> Operation (ç›´é€šè½¦ä¼˜åŒ–) -> Reflection (éªŒæ”¶)

from langchain_community.chat_models import ChatTongyi
from typing import List, TypedDict, Annotated, Literal
from loguru import logger
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
import operator
import json


# ==================== State å®šä¹‰ ====================

class AIOpsState(TypedDict):
    """AIOps Agent çŠ¶æ€å®šä¹‰"""
    input: str
    plan: str
    past_steps: Annotated[List[str], operator.add]
    iteration: int
    response: str


# ==================== AIOps Agent ç±» ====================

class AIOpsAgent:
    """AIOps Agent - è‡ªåŠ¨åŒ–æ•…éšœæ’æŸ¥ä¸“å®¶"""

    def __init__(self, api_key: str, model: str, tools: List):
        self.base_llm = ChatTongyi(
            dashscope_api_key=api_key,
            model_name=model,
            streaming=False,
            temperature=0.1
        )
        self.tool_llm = self.base_llm.bind_tools(tools)
        self.tools = {tool.name: tool for tool in tools}
        self.graph = self._create_graph()
        logger.info("AIOpsAgent åˆå§‹åŒ–æˆåŠŸ (æ˜¾ç¤ºä¿®å¤ç‰ˆ)")

    def _create_graph(self):
        workflow = StateGraph(AIOpsState)
        workflow.add_node("planner", self.planner_node)
        workflow.add_node("operation", self.operation_node)
        workflow.add_node("reflection", self.reflection_node)

        workflow.set_entry_point("planner")
        workflow.add_edge("planner", "operation")
        workflow.add_edge("operation", "reflection")
        workflow.add_conditional_edges(
            "reflection",
            self.should_continue,
            {
                "continue": "planner",
                "end": END
            }
        )
        return workflow.compile()

    # ==================== 1. Planner èŠ‚ç‚¹ ====================

    async def planner_node(self, state: AIOpsState):
        logger.info("ğŸ§  Planner: å¼€å§‹åˆ¶å®šè®¡åˆ’")
        user_input = state["input"]
        past_steps = state.get("past_steps", [])
        iteration = state.get("iteration", 0)

        # --- 1. å…³é”®çŠ¶æ€ç»Ÿè®¡ ---
        has_checked_alerts = any("query_prometheus_alerts" in step for step in past_steps)
        log_query_count = sum(1 for step in past_steps if "query_log" in step)
        has_checked_docs = any("query_internal_docs" in step for step in past_steps)

        # --- 2. å¼ºåˆ¶çŠ¶æ€æœºé€»è¾‘ ---
        if not has_checked_alerts:
            # ã€é˜¶æ®µ 0ã€‘ï¼šæŸ¥å‘Šè­¦
            plan_instruction = "1. ä¼˜å…ˆè°ƒç”¨ `query_prometheus_alerts` æŸ¥è¯¢å½“å‰å‘Šè­¦ã€‚"

        elif log_query_count == 0:
            # ã€é˜¶æ®µ 1ã€‘ï¼šæŸ¥æ—¥å¿—
            plan_instruction = """1. âœ… å·²è·å–å‘Šè­¦ä¿¡æ¯ã€‚
2. **ä¸‹ä¸€æ­¥å¿…é¡»æŸ¥è¯¢æ—¥å¿—**ã€‚
3. è¯·æ ¹æ®å‘Šè­¦åç§°ï¼Œä¸¥æ ¼å¯¹ç…§ä»¥ä¸‹æ˜ å°„è°ƒç”¨ `query_log`ï¼š
   - HighCPU -> query="cpu"
   - HighMemory/OOM -> query="memory"
   - SlowResponse -> query="slow"
   - Error/Crash -> query="error" """

        elif log_query_count == 1:
            # ã€é˜¶æ®µ 2ã€‘ï¼šæ—¥å¿—é‡è¯•
            plan_instruction = """1. âš ï¸ ä¸Šä¸€æ¬¡æ—¥å¿—æŸ¥è¯¢å¯èƒ½æœªå‘½ä¸­ã€‚
2. **è¯·å°è¯•æ›´æ¢å…³é”®è¯**å†æ¬¡è°ƒç”¨ `query_log`ã€‚
   - å¦‚ "cpu" -> "thread" æˆ– "stack trace"ã€‚
   - å¦‚ "error" -> "exception" æˆ– "fatal"ã€‚"""

        elif log_query_count >= 3 and not has_checked_docs:
            # ã€é˜¶æ®µ 3ã€‘ï¼šå¼ºåˆ¶æŸ¥çŸ¥è¯†åº“
            plan_instruction = """1. ğŸ›‘ æ—¥å¿—æŸ¥è¯¢ç»“æŸã€‚
2. **å¿…é¡»**è°ƒç”¨ `query_internal_docs` æŸ¥è¯¢çŸ¥è¯†åº“ã€‚
   - æå–ä¹‹å‰çš„å‘Šè­¦åï¼ˆå¦‚ HighCPUï¼‰ä½œä¸ºå‚æ•°æŸ¥è¯¢æ’æŸ¥æ‰‹å†Œã€‚"""

        else:
            # ã€é˜¶æ®µ 4ã€‘ï¼šæœ€ç»ˆæ€»ç»“ (è¿™é‡ŒåŠ äº†ä¸€å¥å…³é”®æŒ‡ä»¤ï¼Œç¡®ä¿æ ¼å¼ç»Ÿä¸€)
            plan_instruction = """1. å·²æ”¶é›†å…¨é‡ä¿¡æ¯ã€‚
2. è¯·åŸºäºæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆåˆ†æã€‚
3. **å¿…é¡»ä»¥ 'ã€æœ€ç»ˆæŠ¥å‘Šã€‘' å¼€å¤´** (è¿™å¾ˆé‡è¦ï¼Œå¦åˆ™ç³»ç»Ÿæ— æ³•è¯†åˆ«)ã€‚"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ã€‚
ç”¨æˆ·é—®é¢˜ï¼š{user_input}
è¿›åº¦ï¼šå‘Šè­¦({has_checked_alerts}) -> æ—¥å¿—({log_query_count}) -> çŸ¥è¯†åº“({has_checked_docs})
å†å²ï¼š
{chr(10).join(past_steps[-6:])}

ã€æŒ‡ä»¤ã€‘
{plan_instruction}

è¯·è¾“å‡ºä¸‹ä¸€æ­¥è®¡åˆ’ã€‚"""

        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼éµå®ˆæ’æŸ¥æµç¨‹çš„ä¸“å®¶ã€‚"),
            HumanMessage(content=prompt)
        ]
        response = await self.base_llm.ainvoke(messages)
        plan = response.content

        # å…œåº•
        if not plan: plan = "1. è°ƒç”¨ query_internal_docs æŸ¥è¯¢çŸ¥è¯†åº“ã€‚"

        logger.info(f"ğŸ“‹ Planner æŒ‡ä»¤:\n{plan}")
        return {"plan": plan, "iteration": iteration + 1}

    # ==================== 2. Operation èŠ‚ç‚¹ (å…³é”®ä¿®å¤ç‚¹) ====================

    async def operation_node(self, state: AIOpsState):
        """Operation - æ‰§è¡Œæ“ä½œ"""
        logger.info("âš™ï¸ Operation: æ‰§è¡Œæ“ä½œ")
        plan = state["plan"]

        # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šå¦‚æœ Planner å·²ç»ç”Ÿæˆäº†æœ€ç»ˆæŠ¥å‘Šï¼Œç›´æ¥é€ä¼ ï¼
        # ä¸è¦è®© Tool LLM å†å¤„ç†ä¸€éï¼Œå¦åˆ™å®ƒä¼šæŠŠæŠ¥å‘Šå†…å®¹åæ‰ï¼Œå¯¼è‡´ Reflection æ²¡ä¸œè¥¿çœ‹ã€‚
        if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in plan:
            logger.info("ğŸš€ æ£€æµ‹åˆ°æœ€ç»ˆæŠ¥å‘Šï¼Œç›´é€š Reflection")
            return {"past_steps": [plan]}

        past_steps = state.get("past_steps", [])
        context_str = "\n".join(past_steps[-3:]) if past_steps else "æ— "

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ‰§è¡ŒåŠ©æ‰‹ã€‚
ã€æ’æŸ¥è®¡åˆ’ã€‘
{plan}
ã€ä¸Šä¸‹æ–‡ã€‘
{context_str}
è¯·æ ¹æ®è®¡åˆ’è°ƒç”¨å·¥å…·ã€‚å¦‚æœè®¡åˆ’æ˜¯çº¯æ–‡æœ¬åˆ†æï¼Œè¯·ä¸è¦è°ƒç”¨å·¥å…·ã€‚"""

        messages = [
            SystemMessage(content="ä¸¥æ ¼æŒ‰è®¡åˆ’æ‰§è¡Œã€‚"),
            HumanMessage(content=prompt)
        ]

        response = await self.tool_llm.ainvoke(messages)

        if not hasattr(response, 'tool_calls') or not response.tool_calls:
            return {"past_steps": [f"æ‰§è¡Œç»“æœ(æ— å·¥å…·): {response.content}"]}

        results = []
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            logger.info(f"  ğŸ”§ è°ƒç”¨: {tool_name} | å‚æ•°: {tool_args}")
            try:
                tool = self.tools[tool_name]
                result = await tool.ainvoke(tool_args)
                step_result = f"### ğŸ› ï¸ å·¥å…·: {tool_name}\nå‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}\nç»“æœ: {str(result)[:600]}..."
                results.append(step_result)
            except Exception as e:
                results.append(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")

        return {"past_steps": results}

    # ==================== 3. Reflection èŠ‚ç‚¹ (æ˜¾ç¤ºé€»è¾‘ä¿®å¤) ====================

    async def reflection_node(self, state: AIOpsState):
        """Reflection - è¯„ä¼°ç»“æœ"""
        logger.info("ğŸ¤” Reflection: è¯„ä¼°ä¸­...")

        past_steps = state.get("past_steps", [])
        past_steps_str = "\n\n".join(past_steps)
        last_step = past_steps[-1] if past_steps else ""

        has_logs = any("query_log" in s for s in past_steps)

        # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ Operation ç›´é€šè¿‡æ¥çš„æœ€ç»ˆæŠ¥å‘Š
        if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in last_step:
            # ä¾ç„¶ä¿ç•™é˜²æ—©é€€æ£€æŸ¥ï¼ˆè™½ç„¶èµ°åˆ°è¿™ä¸€æ­¥é€šå¸¸éƒ½æ˜¯å®‰å…¨çš„ï¼‰
            if not has_logs:
                return {}
            # ç›´æ¥æŠŠè¿™ä»½æŠ¥å‘Šä½œä¸ºæœ€ç»ˆ response è¿”å›ï¼
            return {"response": last_step}

        # ä¸‹é¢æ˜¯å¸¸è§„çš„ LLM è¯„ä¼°é€»è¾‘ (ç”¨äºä¸­é—´æ­¥éª¤)
        prompt = f"""è¯„ä¼°å½“å‰è¿›åº¦ã€‚
å·²æ‰§è¡Œæ­¥éª¤ï¼š
{past_steps_str}

é€»è¾‘ï¼š
1. æ²¡æŸ¥æ—¥å¿— -> å›å¤ "éœ€æŸ¥æ—¥å¿—"ã€‚
2. æ²¡æŸ¥çŸ¥è¯†åº“ -> å›å¤ "éœ€æŸ¥çŸ¥è¯†åº“"ã€‚
3. çœ‹åˆ° "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" -> é‡å¤è¾“å‡ºè¯¥æŠ¥å‘Šå†…å®¹ã€‚
"""
        messages = [HumanMessage(content=prompt)]
        response = await self.base_llm.ainvoke(messages)
        evaluation = response.content

        # è¿™é‡Œçš„é˜²æ—©é€€æ˜¯ä¸ºäº†é˜²æ­¢ LLM å¹»è§‰
        if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in evaluation and not has_logs:
            return {}

        if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in evaluation:
            return {"response": evaluation}

        return {}

    # ==================== æ§åˆ¶ä¸å…¥å£ ====================

    def should_continue(self, state: AIOpsState) -> Literal["continue", "end"]:
        if state.get("response"): return "end"
        if state.get("iteration", 0) >= 6: return "end"
        return "continue"

    async def analyze(self, problem: str) -> str:
        try:
            res = await self.graph.ainvoke({
                "input": problem, "plan": "", "past_steps": [], "iteration": 0, "response": ""
            })
            # å¦‚æœ response æœ‰å€¼ï¼Œè¯´æ˜æ­£å¸¸ç»“æŸï¼Œç›´æ¥æ˜¾ç¤º
            return res.get("response") or "ã€æœ€ç»ˆæŠ¥å‘Šã€‘\nè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œå»ºè®®äººå·¥ä»‹å…¥ã€‚"
        except Exception as e:
            return f"Error: {e}"
