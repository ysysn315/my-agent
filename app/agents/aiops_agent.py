# AIOps Agent - è‡ªåŠ¨åŒ–è¿ç»´æ•…éšœæ’æŸ¥
# ä½¿ç”¨ Planner-Operation-Reflection æ¨¡å¼

from langchain_community.chat_models import ChatTongyi
from typing import List, TypedDict, Annotated, Literal
from loguru import logger
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import operator
import json


# ==================== State å®šä¹‰ ====================

class AIOpsState(TypedDict):
    """AIOps Agent çŠ¶æ€å®šä¹‰
    
    State åœ¨ Planner-Operation-Reflection å¾ªç¯ä¸­ä¼ é€’ï¼Œä¿å­˜æ‰€æœ‰å¿…è¦ä¿¡æ¯
    """
    # ç”¨æˆ·è¾“å…¥
    input: str  # ç”¨æˆ·çš„é—®é¢˜æˆ–å‘Šè­¦æè¿°

    # æ’æŸ¥è®¡åˆ’
    plan: str  # Planner ç”Ÿæˆçš„æ’æŸ¥è®¡åˆ’ï¼ˆæ­¥éª¤åˆ—è¡¨ï¼‰

    # æ‰§è¡Œå†å²ï¼ˆä½¿ç”¨ operator.add è‡ªåŠ¨åˆå¹¶ï¼‰
    past_steps: Annotated[List[str], operator.add]  # å·²æ‰§è¡Œçš„æ­¥éª¤å’Œç»“æœ

    # å¾ªç¯æ§åˆ¶
    iteration: int  # å½“å‰å¾ªç¯æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼Œæœ€å¤§3æ¬¡ï¼‰

    # æœ€ç»ˆè¾“å‡º
    response: str  # æœ€ç»ˆçš„åˆ†ææŠ¥å‘Š


# ==================== AIOps Agent ç±» ====================

class AIOpsAgent:
    """AIOps Agent - è‡ªåŠ¨åŒ–æ•…éšœæ’æŸ¥å’Œæ ¹å› åˆ†æ
    
    ä½¿ç”¨ Planner-Operation-Reflection æ¨¡å¼:
    - Planner: åˆ¶å®šæ’æŸ¥è®¡åˆ’
    - Operation: æ‰§è¡Œæ“ä½œï¼ˆè°ƒç”¨å·¥å…·ï¼‰
    - Reflection: åæ€è¯„ä¼°ï¼Œå†³å®šæ˜¯å¦ç»§ç»­
    """

    def __init__(self, api_key: str, model: str, tools: List):
        """
        åˆå§‹åŒ– AIOps Agent
        
        å‚æ•°:
            api_key: DashScope API Key
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "qwen-max"
            tools: å·¥å…·åˆ—è¡¨ [query_prometheus_alerts, query_log, query_internal_docs, get_current_datetime]
        """
        # åˆå§‹åŒ– LLM å¹¶ç»‘å®šå·¥å…·
        self.llm = ChatTongyi(
            dashscope_api_key=api_key,
            model_name=model,
            streaming=False
        ).bind_tools(tools)

        # ä¿å­˜å·¥å…·å­—å…¸ï¼ˆæ–¹ä¾¿åç»­è°ƒç”¨ï¼‰
        self.tools = {tool.name: tool for tool in tools}

        # åˆ›å»º Graph
        self.graph = self._create_graph()

        logger.info("AIOpsAgent åˆå§‹åŒ–æˆåŠŸ")

    def _create_graph(self):
        """åˆ›å»º LangGraph å·¥ä½œæµ
        
        å·¥ä½œæµ: Planner â†’ Operation â†’ Reflection â†’ (ç»§ç»­ or ç»“æŸ)
        """
        # TODO: å®šä¹‰èŠ‚ç‚¹å‡½æ•°
        # TODO: æ„å»ºå›¾
        workflow = StateGraph(AIOpsState)
        workflow.add_node("planner", self.planner_node)
        workflow.add_node("operation", self.operation_node)
        workflow.add_node("reflection", self.reflection_node)
        workflow.set_entry_point("planner")
        workflow.add_edge("planner", "operation")
        workflow.add_edge("operation", "reflection")
        workflow.add_conditional_edges(
            "reflection",
            self.should_continue,
            {
                "continue": "planner",
                "end": END
            }
        )
        return workflow.compile()

    # ==================== èŠ‚ç‚¹å‡½æ•° ====================

    async def planner_node(self, state: AIOpsState):
        """Planner - åˆ¶å®šæ’æŸ¥è®¡åˆ’
        
        åŠŸèƒ½:
        - åˆ†æé—®é¢˜
        - åˆ¶å®šæ’æŸ¥æ­¥éª¤
        - å†³å®šè°ƒç”¨å“ªäº›å·¥å…·
        """
        logger.info("ğŸ§  Planner: å¼€å§‹åˆ¶å®šæ’æŸ¥è®¡åˆ’")

        # 1. è·å–å½“å‰çŠ¶æ€
        user_input = state["input"]
        past_steps = state.get("past_steps", [])
        iteration = state.get("iteration", 0)

        # 2. æ„å»º Prompt
        if iteration == 0:
            # ç¬¬ä¸€æ¬¡è§„åˆ’
            prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£åˆ¶å®šæ•…éšœæ’æŸ¥è®¡åˆ’ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

è¯·åˆ¶å®šè¯¦ç»†çš„æ’æŸ¥è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š
1. éœ€è¦è°ƒç”¨å“ªäº›å·¥å…·æ¥æ”¶é›†ä¿¡æ¯
2. æ¯ä¸ªå·¥å…·çš„å‚æ•°
3. æ’æŸ¥çš„ç†ç”±

å¯ç”¨å·¥å…·ï¼š
- query_prometheus_alerts: æŸ¥è¯¢ Prometheus å‘Šè­¦
- query_log: æŸ¥è¯¢ç³»ç»Ÿæ—¥å¿—ï¼ˆå‚æ•°ï¼šquery, time_rangeï¼‰
- query_internal_docs: æŸ¥è¯¢è¿ç»´çŸ¥è¯†åº“ï¼ˆå‚æ•°ï¼šqueryï¼‰
- get_current_datetime: è·å–å½“å‰æ—¶é—´

è¯·ä»¥æ¸…æ™°çš„æ­¥éª¤åˆ—è¡¨å½¢å¼è¾“å‡ºè®¡åˆ’ã€‚"""
        else:
            # é‡æ–°è§„åˆ’ï¼ˆReplanï¼‰
            past_steps_str = "\n".join(past_steps)
            prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£åˆ¶å®šæ•…éšœæ’æŸ¥è®¡åˆ’ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

å·²æ‰§è¡Œçš„æ­¥éª¤ï¼š
{past_steps_str}

æ ¹æ®å·²æœ‰ä¿¡æ¯ï¼Œè¯·åˆ¶å®šä¸‹ä¸€æ­¥çš„æ’æŸ¥è®¡åˆ’ã€‚å¦‚æœä¿¡æ¯å·²ç»è¶³å¤Ÿï¼Œè¯·è¯´æ˜"ä¿¡æ¯å……è¶³ï¼Œå¯ä»¥ç”ŸæˆæŠ¥å‘Š"ã€‚

å¯ç”¨å·¥å…·ï¼š
- query_prometheus_alerts: æŸ¥è¯¢ Prometheus å‘Šè­¦
- query_log: æŸ¥è¯¢ç³»ç»Ÿæ—¥å¿—ï¼ˆå‚æ•°ï¼šquery, time_rangeï¼‰
- query_internal_docs: æŸ¥è¯¢è¿ç»´çŸ¥è¯†åº“ï¼ˆå‚æ•°ï¼šqueryï¼‰
- get_current_datetime: è·å–å½“å‰æ—¶é—´

è¯·ä»¥æ¸…æ™°çš„æ­¥éª¤åˆ—è¡¨å½¢å¼è¾“å‡ºè®¡åˆ’ã€‚"""

        # 3. è°ƒç”¨ LLM ç”Ÿæˆè®¡åˆ’
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AIOps æ•…éšœæ’æŸ¥ä¸“å®¶ã€‚"),
            HumanMessage(content=prompt)
        ]

        response = await self.llm.ainvoke(messages)
        plan = response.content

        logger.info(f"ğŸ“‹ Planner ç”Ÿæˆè®¡åˆ’:\n{plan}")

        # 4. æ›´æ–° State
        return {
            "plan": plan,
            "iteration": iteration + 1
        }

    async def operation_node(self, state: AIOpsState):
        """Operation - æ‰§è¡Œæ“ä½œ
        
        åŠŸèƒ½:
        - æ‰§è¡Œè®¡åˆ’ä¸­çš„æ­¥éª¤
        - è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯
        - è®°å½•æ‰§è¡Œç»“æœ
        """
        logger.info("âš™ï¸ Operation: å¼€å§‹æ‰§è¡Œæ“ä½œ")

        # 1. è·å–è®¡åˆ’
        plan = state["plan"]
        user_input = state["input"]

        # 2. æ„å»º Promptï¼ˆè®© LLM æ ¹æ®è®¡åˆ’è°ƒç”¨å·¥å…·ï¼‰
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps æ‰§è¡ŒåŠ©æ‰‹ï¼Œè´Ÿè´£è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

æ’æŸ¥è®¡åˆ’ï¼š
{plan}

è¯·æ ¹æ®è®¡åˆ’è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚ä½ å¿…é¡»è°ƒç”¨ä»¥ä¸‹å·¥å…·ä¹‹ä¸€ï¼š
- query_prometheus_alerts: æŸ¥è¯¢å½“å‰å‘Šè­¦ï¼ˆæ— éœ€å‚æ•°ï¼‰
- query_log: æŸ¥è¯¢æ—¥å¿—ï¼ˆå‚æ•°ï¼šquery, time_rangeï¼‰
- query_internal_docs: æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆå‚æ•°ï¼šqueryï¼‰
- get_current_datetime: è·å–å½“å‰æ—¶é—´ï¼ˆæ— éœ€å‚æ•°ï¼‰

é‡è¦ï¼šè¯·æ ¹æ®è®¡åˆ’è°ƒç”¨åˆé€‚çš„å·¥å…·ï¼Œä¸è¦åªè°ƒç”¨ get_current_datetimeï¼"""

        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ª AIOps æ‰§è¡ŒåŠ©æ‰‹ï¼Œè´Ÿè´£è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯ã€‚"),
            HumanMessage(content=prompt)
        ]

        # 3. è°ƒç”¨ LLM
        response = await self.llm.ainvoke(messages)

        # 4. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if not hasattr(response, 'tool_calls') or not response.tool_calls:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å› LLM çš„å›ç­”
            logger.warning("âš ï¸ Operation: LLM æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·")
            return {
                "past_steps": [f"æ‰§è¡Œç»“æœ: {response.content}"]
            }

        # 5. æ‰§è¡Œæ‰€æœ‰å·¥å…·
        logger.info(f"ğŸ”§ Operation: æ‰§è¡Œ {len(response.tool_calls)} ä¸ªå·¥å…·")

        results = []
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]

            logger.info(f"  - è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

            try:
                # è·å–å·¥å…·å¹¶æ‰§è¡Œ
                tool = self.tools[tool_name]
                result = await tool.ainvoke(tool_args)  # â† å…³é”®ï¼šéœ€è¦ await

                # è®°å½•ç»“æœï¼ˆæˆªæ–­è¿‡é•¿ç»“æœï¼‰
                step_result = f"å·¥å…·: {tool_name}\nå‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}\nç»“æœ: {result[:500]}..."
                results.append(step_result)

                logger.info(f"  âœ… {tool_name} æ‰§è¡ŒæˆåŠŸ")
            except Exception as e:
                error_msg = f"å·¥å…·: {tool_name}\né”™è¯¯: {str(e)}"
                results.append(error_msg)
                logger.error(f"  âŒ {tool_name} æ‰§è¡Œå¤±è´¥: {e}")

        # 6. è¿”å›ç»“æœï¼ˆä¼šè‡ªåŠ¨æ·»åŠ åˆ° past_stepsï¼‰
        return {
            "past_steps": results
        }

    async def reflection_node(self, state: AIOpsState):
        """Reflection - åæ€è¯„ä¼°
        
        åŠŸèƒ½:
        - è¯„ä¼°æ‰§è¡Œç»“æœ
        - åˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……è¶³
        - å¦‚æœå……è¶³ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        - å¦‚æœä¸è¶³ï¼Œä¸åšä»»ä½•æ“ä½œï¼ˆè®© should_continue å†³å®šï¼‰
        """
        logger.info("ğŸ¤” Reflection: å¼€å§‹è¯„ä¼°ç»“æœ")

        # 1. è·å–çŠ¶æ€
        user_input = state["input"]
        past_steps = state.get("past_steps", [])
        iteration = state.get("iteration", 0)

        # 2. æ„å»º Prompt
        past_steps_str = "\n\n".join(past_steps)
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AIOps ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°æ•…éšœæ’æŸ¥ç»“æœã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
{past_steps_str}

è¯·è¯„ä¼°å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿè¿›è¡Œæ ¹å› åˆ†æï¼š

1. å¦‚æœä¿¡æ¯å……è¶³ï¼š
   - è¯·ç”Ÿæˆè¯¦ç»†çš„æ•…éšœåˆ†ææŠ¥å‘Š
   - æŠ¥å‘Šæ ¼å¼ï¼šé—®é¢˜æè¿°ã€æ ¹å› åˆ†æã€è§£å†³å»ºè®®
   - ä»¥"ã€æœ€ç»ˆæŠ¥å‘Šã€‘"å¼€å¤´

2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼š
   - ç®€å•è¯´æ˜"ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç»§ç»­æ’æŸ¥"
   - ä¸è¦ç»™å‡ºå…·ä½“å»ºè®®ï¼ˆPlanner ä¼šé‡æ–°è§„åˆ’ï¼‰"""

        # 3. è°ƒç”¨ LLM
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AIOps æ•…éšœåˆ†æä¸“å®¶ã€‚"),
            HumanMessage(content=prompt)
        ]

        response = await self.llm.ainvoke(messages)
        evaluation = response.content

        logger.info(f"ğŸ“Š Reflection è¯„ä¼°:\n{evaluation[:200]}...")

        # 4. å¦‚æœç”Ÿæˆäº†æœ€ç»ˆæŠ¥å‘Šï¼Œä¿å­˜åˆ° response
        if "ã€æœ€ç»ˆæŠ¥å‘Šã€‘" in evaluation:
            logger.info("âœ… Reflection: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
            return {
                "response": evaluation
            }
        else:
            # ä¿¡æ¯ä¸è¶³ï¼Œä¸æ›´æ–° responseï¼ˆä¿æŒä¸ºç©ºï¼‰
            logger.info("ğŸ”„ Reflection: ä¿¡æ¯ä¸è¶³")
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œä¸æ›´æ–°ä»»ä½•å­—æ®µ

    # ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

    def should_continue(self, state: AIOpsState) -> Literal["continue", "end"]:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯
        
        å†³ç­–é€»è¾‘ï¼š
        1. å¦‚æœ response ä¸ä¸ºç©º â†’ å·²ç”ŸæˆæŠ¥å‘Š â†’ end
        2. å¦‚æœ iteration >= 3 â†’ è¾¾åˆ°ä¸Šé™ â†’ endï¼ˆå¼ºåˆ¶ï¼‰
        3. å¦åˆ™ â†’ continue
        
        è¿”å›:
            "continue": å›åˆ° Planner ç»§ç»­æ’æŸ¥
            "end": ç»“æŸæµç¨‹
        """
        response = state.get("response", "")
        iteration = state.get("iteration", 0)

        # 1. å¦‚æœå·²ç»ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œç»“æŸ
        if response:
            logger.info("ğŸ¯ å†³ç­–: å·²ç”ŸæˆæŠ¥å‘Šï¼Œç»“æŸæµç¨‹")
            return "end"

        # 2. å¦‚æœè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
        if iteration >= 3:
            logger.warning("âš ï¸ å†³ç­–: è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°(3æ¬¡)ï¼Œå¼ºåˆ¶ç»“æŸ")
            return "end"

        # 3. å¦åˆ™ç»§ç»­
        logger.info(f"ğŸ”„ å†³ç­–: ç»§ç»­æ’æŸ¥ï¼ˆå½“å‰ç¬¬ {iteration} è½®ï¼‰")
        return "continue"

    # ==================== å¯¹å¤–æ¥å£ ====================

    async def analyze(self, problem: str) -> str:
        """
        åˆ†ææ•…éšœå¹¶ç”ŸæˆæŠ¥å‘Š
        
        å‚æ•°:
            problem: æ•…éšœæè¿°æˆ–å‘Šè­¦ä¿¡æ¯
        
        è¿”å›:
            åˆ†ææŠ¥å‘Š
        """
        try:
            logger.info(f"ğŸš€ AIOps Agent å¼€å§‹åˆ†æé—®é¢˜: {problem[:100]}...")
            initial_state = {
                "input": problem,
                "plan": "",
                "past_steps": [],
                "iteration": 0,
                "response": ""
            }
            result = await self.graph.ainvoke(initial_state)

            final_report = result.get("response", "")

            if not final_report:
                # å¦‚æœæ²¡æœ‰ç”ŸæˆæŠ¥å‘Šï¼ˆè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•æŠ¥å‘Š
                logger.warning("âš ï¸ æœªç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå¯èƒ½è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°")
                past_steps_str = "\n\n".join(result.get("past_steps", []))
                final_report = f"""ã€åˆ†ææŠ¥å‘Šã€‘
                é—®é¢˜æè¿°ï¼š{problem}

å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
{past_steps_str}

æ³¨æ„ï¼šç”±äºè¾¾åˆ°æœ€å¤§æ’æŸ¥æ¬¡æ•°é™åˆ¶ï¼Œåˆ†æå¯èƒ½ä¸å®Œæ•´ã€‚å»ºè®®äººå·¥ä»‹å…¥è¿›ä¸€æ­¥æ’æŸ¥ã€‚"""
            logger.info("âœ… AIOps Agent åˆ†æå®Œæˆ")
            return final_report
        except Exception as e:
            logger.error(f"âŒ AIOps Agent åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            raise
